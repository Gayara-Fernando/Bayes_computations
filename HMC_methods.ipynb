{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d9c4574-692a-4a63-9ed3-ee2feaf50121",
   "metadata": {},
   "source": [
    "HMC with https://colab.research.google.com/drive/1YQBSfS1Nb8a9TAMsV1RjWsiErWqXLbrj#scrollTo=FqU8oNOCTcdL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f55b7b-c229-43aa-9bb8-864b21a99490",
   "metadata": {},
   "source": [
    "Hamiltonian Monte Carlo is a variant of MH, but the proposal distribution adds more complexity. MH uses random walks to explore the probability space; as discussed, the likelihoods of current and proposed locations are compared for movement criteria. However, as dimensionality increases, the high likelihood region(s) composes (exponentially) less and less of the total area. In these cases, MH cannot efficiently explore all regions of the distribution and might return samples that are not truly representative of the distribution's shape.\n",
    "\n",
    "HMC instead designs a scheme by which the size of the jump is proportionate to the (negative log) likelihood of the sampler's current position. When close to the high likelihood region(s), it proposes nearby positions; conversely, when far from these high likelihood region(s), larger jumps are proposed. The key is relating position to leap size.\n",
    "\n",
    "HMC loosely uses Hamiltonian mechanics to inform these jumps; namely, differential equations relating position and momentum to each other. Often, differential equations cannot be solved exactly, however various symplectic integration methods (such as leapfrog integration) can closely approximate the solution. These (differential) equations are: (A) the gradient of position with respect to time and (B) the gradient of momentum with respect to time. Leapfrog integration iteratively updates position and momentum in order to estimate where a particle will be given its current position and momentum.\n",
    "\n",
    "Variables and gradients are as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78193f1-d483-4da0-ae29-2a4516213ed1",
   "metadata": {},
   "source": [
    "Q = position (parameters in bayes analogy)\n",
    "P = momentum \n",
    "V = potential energy\n",
    "K = kinetic energy\n",
    "T = times\n",
    "dQ/dT = P # position wrt time equals momentum\n",
    "dP/dT = -dV/dQ # momentum wrt time equals potential energy wrt position"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6af7884-20d7-4a6e-b836-59df6640d6fc",
   "metadata": {},
   "source": [
    "Of note, position is updated by momentum, and momentum is updated by the change in potential energy wrt position. In our context, position means the specific input variable, x, in the neagtive log PDF. When the position is approaching the high likelihood region(s), the slope is near 0. But when far from said region(s), the slope is much steeper. The effect is that positions corresponding to moderate and low likelihoods experience a large gradient in the direction of the highest likelihood region(s). This has the beneficial effect of taking big jumps when exploring moderate/low likelihood regions (most of the distribution) but small jumps when exploring high likelihood region(s). Refer to above graphics to conceptually consider why this might be beneficial.\n",
    "\n",
    "The negative log PDF is used for multiple reasons: First, (in the context of the univariate gaussian) the log function undoes exponentiation, resulting in a parabolla; this means the slope becomes increaseingly steeper with distance from the expectation (no inflection points, tail slopes don't approach 0). Second, logarithms tend to limit the risk of numerical underflow and overflow. And third, logarithms tend to simplify differentiation, resulting in a less complex functions, which are easier to derive.\n",
    "\n",
    "As discussed in graphics above, if the step size and integration length are well tuned to the current position, the ball will roll down the bowl and back up to the same height on the opposing side; however, the sampler proposes new positions at the end of the integration period, not at intermediary steps in between; this has the effect of bouncing between positions at (or near) the same likelihood. In order to explore other likelihoods, a random increase or decrease in momentum is needed.\n",
    "\n",
    "These \"momentum kicks\" take the form of the proposal distribution in the familiar context of the MH algorithm. A kick is sampled, leapfrog integration uses hamiltonian mechanics to determine how far the sampler will \"travel\" (given a fixed integration interval.) It might not be obvious, but the proposal distribution is not yet symmeytric. If a ball rolls down the parabolla, it starts in a high position with high potential energy but low kinetic energy. As it rolls down the curve, it now has high kinetic energy but low potential energy. If the kinetic energy were to be spontaneously reversed, it would ensure that the ball would travel from point B, at the bottom of the parabolla, back up to its starting position, point A. And this correction of \"flipping\" the momentum ensures proposal distribution symmetry.\n",
    "\n",
    "In other words, to correct for the assymetric proposal distrbiution, the starting position and starting momentum is taken and compared to the ending position and negative ending kinetic energy, such that the motion could be reversed. With this correction, the sampler now behaves as though it had a symmetric proposal distribution. The math in the code below appears a bit tricky but this is solely due to the use of negative logarithms (ie division becomes subtraction, multiplication becomes addition and due to the negative log, the signs are reversed.)\n",
    "\n",
    "Inm previous sections, the exponential distribution was sampled using the beta distribution and the \"hastings ratio\" was computed to correct for the assyemtric distribution. Study this concept to better understand the hastings ratio in the context of HMC.\n",
    "\n",
    "Of note, the step size and integration length variables in the code below inform the HMC sampler for how long to travel given some starting momentum and position. In general, it is very difficult to \"tune the proposal distribution\", meaning find a step size and integration length that suits generally any position/likelihood, which the sampler might explore.\n",
    "\n",
    "The slightest change to these variables can negatively affect HMC performance. For example, the hypothetical ball could start on the negative log gaussian slope above, roll all the ways to the top of the other side, and back again to the original position. This is known as a \"U-turn\" and is the basis for the aptly named \"No U-Turn Sampler\" or NUTS. In practice, there are no \"one size fits all\" step size and path length parameters. As such, NUTS is more common in practice than HMC.\n",
    "\n",
    "Disclaimer: All these were copied from the blog in the given link."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95f64ce-6618-4319-a321-39466825c64e",
   "metadata": {},
   "source": [
    "Let's look at the implementation and try to understand what all is happening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8ecd53-4366-4eb1-9bd1-511cb46dfae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import scipy.stats as st\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5cbfb95-76de-4775-9870-1a99acebd384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the normal pdf for a single value(I think could be a vector as well) as follows\n",
    "def normal(x,mu,sigma):\n",
    "    numerator = np.exp(-1*((x-mu)**2)/(2*sigma**2))\n",
    "    denominator = sigma * np.sqrt(2*np.pi)\n",
    "    return numerator/denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c6171f-9c2f-42e3-b6f4-42839c07b885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# negative log pdf\n",
    "def neg_log_prob(x,mu,sigma):\n",
    "    return -1*np.log(normal(x=x,mu=mu,sigma=sigma)) # note this normal is coming from the earlier function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f765eec8-6fe6-47ef-8a75-62a6f8aef53c",
   "metadata": {},
   "source": [
    "##### The leap frog in this example seems incorrect, therefore, let's stick to the one from ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7025e3a6-1465-4dad-89ed-e2a9d0a40b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hmc function\n",
    "def HMC(mu = 0.1, sigma = 1.0, path_len = 1, step_size = 0.25, initial_position = 0.0, epochs = 1000):\n",
    "    # mu, sigma for teh normal pdf above\n",
    "    # what is path_len though?\n",
    "    # step_size is step size for the algorithm\n",
    "    # initial_position - initial value of x?\n",
    "    # path_len is L? no.of steps?\n",
    "    # epochs is the number of times to run the insides of L? No idea, understand this.\n",
    "\n",
    "    # setup\n",
    "    steps = int(path_len/step_size) # why is this necessary though, we read this in chapter 5 too, that this needs to be an integer. \n",
    "    samples = [initial_position] # this is where we will keep track of the parameter, this is what we are going to update - and then do accept/reject.\n",
    "    momentum_dist = st.norm(0, 1) # this is for the momentum, this will instantiate a standard normal distribution frrom which samples will be drawn later for the momentum variable\n",
    "\n",
    "    # generate samples\n",
    "    for i in range(epochs):\n",
    "        # get the current value \n",
    "        q0 = np.copy(samples[-1])\n",
    "        # copy the curent value as q1 - why though?\n",
    "        q1 = np.copy(q0)\n",
    "        # sample p0 from the specified dsitribtution\n",
    "        p0 = momentum_dist.rvs()\n",
    "        # p1 is the same as p0? - why?\n",
    "        p1 = np.copy(p0)\n",
    "        # get the derivative \n",
    "        dVdQ = -1*(q0 - mu)/(sigma**2)\n",
    "\n",
    "# This implementation seems incorrect - Let's stick to the one from ChatGPT\n",
    "        # leap frog integration\n",
    "        for j in range(steps):\n",
    "            p1 += step_size*dVdQ/2 # as potential energy increases, kinetic energy decreases, half-step\n",
    "            q1 += step_size*p1 # position increases as function of momentum \n",
    "            new_dVdQ = - 1*(q1 - mu)/(sigma**2)\n",
    "            p1 += step_size*new_dVdQ/2 # second half-step \"leapfrog\" update to momentum\n",
    "        # leapfrog integration end        \n",
    "        p1 = -1*p1 #flip momentum for reversibility\n",
    "\n",
    "        #metropolis acceptance\n",
    "        q0_nlp = neg_log_prob(x=q0,mu=mu,sigma=sigma)\n",
    "        q1_nlp = neg_log_prob(x=q1,mu=mu,sigma=sigma)        \n",
    "\n",
    "        p0_nlp = neg_log_prob(x=p0,mu=0,sigma=1)\n",
    "        p1_nlp = neg_log_prob(x=p1,mu=0,sigma=1)\n",
    "\n",
    "        # Account for negatives AND log(probabiltiies)...\n",
    "        target = q0_nlp - q1_nlp # P(q1)/P(q0)\n",
    "        adjustment = p1_nlp - p0_nlp # P(p0)/P(p1)\n",
    "        acceptance = target + adjustment # [P(q1)*P(p0)]/[P(q0)*P(p1)] \n",
    "        \n",
    "        event = np.log(random.uniform(0,1))\n",
    "        if event <= acceptance:\n",
    "            samples.append(q1)\n",
    "        else:\n",
    "            samples.append(q0)\n",
    "    \n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33b238a-0b02-4821-9af9-a831aaff33f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = 0\n",
    "sigma = 1\n",
    "trial = HMC(mu=mu,sigma=sigma,path_len=1.5,step_size=0.25) #note the step_size and path_len parameters\n",
    "\n",
    "lines = np.linspace(-6,6,10_000)\n",
    "normal_curve = [normal(x=l,mu=mu,sigma=sigma) for l in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c594dd6b-b3f9-46a8-b861-2afc6a3e2f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lines,normal_curve)\n",
    "plt.hist(trial,density=True,bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63ed720-4079-4ca5-beaf-6eeede539e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = 0\n",
    "sigma = 2\n",
    "trial = HMC(mu=mu,sigma=sigma,path_len=4,step_size=0.5) #note the change in step_size and path_len parameters...\n",
    "\n",
    "lines = np.linspace(-6,6,10_000)\n",
    "normal_curve = [normal(x=l,mu=mu,sigma=sigma) for l in lines]\n",
    "\n",
    "plt.plot(lines,normal_curve)\n",
    "plt.hist(trial,density=True,bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d439f8-13fe-4deb-b6cd-1a188145a494",
   "metadata": {},
   "source": [
    "We are ready for Hamiltonian Monte Carlo methods. We need to specifically understand the NUTs sampler. The following code is from ChapGPT for the implementation of HMC methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e579d095-5b7a-496c-a344-6abf9e110ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bcf804-af5c-4cd2-9de5-5ef4c162be2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the target distribution (e.g., a standard Gaussian)\n",
    "def target_log_prob(x):\n",
    "    return -0.5 * np.sum(x**2)\n",
    "\n",
    "# Gradient of the target log-probability\n",
    "def grad_log_prob(x):\n",
    "    return -x\n",
    "\n",
    "# Leapfrog integrator\n",
    "def leapfrog(x, p, step_size, num_steps):\n",
    "    x_new = np.copy(x)\n",
    "    p_new = np.copy(p)\n",
    "    \n",
    "    # Half step for momentum at the beginning\n",
    "    p_new -= 0.5 * step_size * grad_log_prob(x_new)\n",
    "    \n",
    "    # Full step for position\n",
    "    for _ in range(num_steps):\n",
    "        x_new += step_size * p_new\n",
    "        # Full step for momentum (except last iteration)\n",
    "        if _ != num_steps - 1:\n",
    "            p_new -= step_size * grad_log_prob(x_new)\n",
    "    \n",
    "    # Half step for momentum at the end\n",
    "    p_new -= 0.5 * step_size * grad_log_prob(x_new)\n",
    "    \n",
    "    return x_new, p_new\n",
    "\n",
    "# Hamiltonian function (H = Kinetic + Potential)\n",
    "def hamiltonian(x, p):\n",
    "    kinetic = 0.5 * np.sum(p**2)  # Kinetic energy from momentum (Gaussian)\n",
    "    potential = -target_log_prob(x)  # Potential energy from target distribution\n",
    "    return kinetic + potential\n",
    "\n",
    "# HMC sampler\n",
    "def hmc_sampler(init_x, step_size, num_steps, num_samples):\n",
    "    samples = []\n",
    "    x = np.copy(init_x)\n",
    "    \n",
    "    for _ in range(num_samples):\n",
    "        # Sample momentum from standard normal\n",
    "        p = np.random.randn(*x.shape)\n",
    "        \n",
    "        # Save the current state\n",
    "        current_x = np.copy(x)\n",
    "        current_p = np.copy(p)\n",
    "        \n",
    "        # Perform leapfrog integration\n",
    "        new_x, new_p = leapfrog(x, p, step_size, num_steps)\n",
    "        \n",
    "        # Calculate Hamiltonians\n",
    "        current_H = hamiltonian(current_x, current_p)\n",
    "        new_H = hamiltonian(new_x, new_p)\n",
    "        \n",
    "        # Metropolis-Hastings correction\n",
    "        if np.random.rand() < np.exp(current_H - new_H):\n",
    "            x = new_x  # Accept the new state\n",
    "        # else: reject and keep the current state\n",
    "        \n",
    "        samples.append(np.copy(x))\n",
    "    \n",
    "    return np.array(samples)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbeb4c26-334a-4ebe-9bce-77d53ffb6ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "init_x = np.array([0.0])  # Initial position\n",
    "step_size = 0.1  # Step size for leapfrog\n",
    "num_steps = 10  # Number of leapfrog steps\n",
    "num_samples = 1000  # Number of samples to generate\n",
    "\n",
    "# Run HMC\n",
    "samples = hmc_sampler(init_x, step_size, num_steps, num_samples)\n",
    "\n",
    "# Print the first few samples\n",
    "print(samples[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8896d5ac-23be-4264-9d09-893c69117824",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905644dd-95d4-40c5-8ce6-8be182aa8fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.hist(samples,density=True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tfp_env_TN_CPU)",
   "language": "python",
   "name": "tfp_env_tn_cpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
